{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prior probability of positive class is 0.5579681007907787\n",
      "The prior probability of negative class is 0.44203189920922126\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from csv import QUOTE_NONE\n",
    "\n",
    "# Read the SST data\n",
    "sst_data = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
    "\n",
    "#Take 100 rows in validation set\n",
    "validation_set = sst_data[:100]\n",
    "\n",
    "#Take 100 rows in test set\n",
    "test_set = sst_data[100:200]\n",
    "\n",
    "#put remaining rows in training set\n",
    "training_set = sst_data[200:]\n",
    "\n",
    "# initialize the positive and negative count\n",
    "positive_count = (training_set['label'] == 1).sum()\n",
    "negative_count = (training_set['label'] == 0).sum()\n",
    "\n",
    "#length of training set\n",
    "training_length = len(training_set)\n",
    "\n",
    "#getting prior probability for positive and negative classes\n",
    "prior_probab_positive = positive_count / training_length\n",
    "prior_probab_negative = negative_count / training_length\n",
    "\n",
    "#printing prior probability of both classes\n",
    "print(f\"The prior probability of positive class is {prior_probab_positive}\")\n",
    "print(f\"The prior probability of negative class is {prior_probab_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenized first sentence is: ['<s>', 'told', 'in', 'scattered', 'fashion', '</s>']\n",
      "The vocabulary size of training set is 14813\n"
     ]
    }
   ],
   "source": [
    "# A function to tokenized sentence and padding with start and end symbols\n",
    "def sentence_tokenizer(sentence):\n",
    "    tokenize_sentence = sentence.split()\n",
    "    tokenize_sentence.insert(0, '<s>')\n",
    "    tokenize_sentence.append('</s>')\n",
    "\n",
    "    return tokenize_sentence\n",
    "\n",
    "#tokenized the entire training dataset\n",
    "tokenized_training_set = training_set['sentence'].apply(sentence_tokenizer)\n",
    "\n",
    "#getting the first line from tokenized training set and printing it\n",
    "first_line_trainingset = tokenized_training_set.iloc[0]\n",
    "print(f\"The tokenized first sentence is: {first_line_trainingset}\")\n",
    "\n",
    "#Getting a list of tokenized sentences in the training set\n",
    "tokenizedlist_trainingset = tokenized_training_set.tolist()\n",
    "\n",
    "total_tokens = []\n",
    "\n",
    "#getting the unique words in total_tokens list\n",
    "for i in tokenizedlist_trainingset:\n",
    "    for j in i:\n",
    "        total_tokens.append(j)\n",
    "\n",
    "vocabulary_set = set(total_tokens)\n",
    "\n",
    "\n",
    "vocabulary_size = len(vocabulary_set)                                       #getting the count of the unique words\n",
    "\n",
    "print(f\"The vocabulary size of training set is {vocabulary_size}\")          #printing the count of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of bigram ('<s>', 'the') = 4450\n"
     ]
    }
   ],
   "source": [
    "# Function to count the biagram frequencies\n",
    "def count_biagram_frequencies(tokenized_sequences):\n",
    "    bigram_counts = {}                                                                  #Initializing a dictionary\n",
    "\n",
    "    for tokens in tokenized_sequences:\n",
    "        for i in range(len(tokens)-1):  \n",
    "            first_word = tokens[i]                                                       # getting the first word\n",
    "            second_word = tokens[i+1]                                                    # getting the second word\n",
    "\n",
    "            bigram = (first_word, second_word)                                              \n",
    "            if bigram not in bigram_counts:                                              # Getting the bigram counts\n",
    "                bigram_counts[bigram]=0\n",
    "            bigram_counts[bigram]+=1\n",
    "    return bigram_counts\n",
    "\n",
    "\n",
    "# Setting the bigram counts value in bigram_counts variable\n",
    "bigram_counts = count_biagram_frequencies(tokenizedlist_trainingset)\n",
    "\n",
    "#printing frequency of bigram '<s>' and 'the'\n",
    "print(f\"Frequency of bigram ('<s>', 'the') = {bigram_counts.get(('<s>', 'the'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability with alpha= 0.001 is  -1.0250904304166832\n",
      "The log probability with alpha= 0.5 is  -6.172912066128204\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def smoothed_biagram_probab(wm, wm_1, bigram_counts, alpha, vocabulary_size):\n",
    "\n",
    "    # count of biagram(wm, wm-1)\n",
    "    count_bigram = bigram_counts.get((wm_1, wm), 0)\n",
    "\n",
    "    #count of unigram(wm-1)\n",
    "    count_unigram = 0\n",
    "    for i, j in bigram_counts.items():\n",
    "        if i[0] == wm_1:\n",
    "            count_unigram +=j\n",
    "\n",
    "    # To get smoothed probability using alpha smoothing formula\n",
    "    smoothed_probability = (count_bigram + alpha) / (count_unigram + alpha * vocabulary_size)\n",
    "\n",
    "    # TO get the negative log probability\n",
    "    calc_negative_log_probab = math.log(smoothed_probability)\n",
    "\n",
    "    return calc_negative_log_probab\n",
    "\n",
    "\n",
    "word = 'award'\n",
    "previous_word = 'academy'\n",
    "# passing the parameters with alpha as 0.001 to the smoothed function to get the negative log probability\n",
    "negative_log_probab1 = smoothed_biagram_probab(word, previous_word, bigram_counts, 0.001, 14813)\n",
    "\n",
    "# passing the parameters with alpha as 0.5 to the smoothed function to get the negative log probability\n",
    "negative_log_probab2 = smoothed_biagram_probab(word, previous_word, bigram_counts, 0.5, 14813)\n",
    "\n",
    "# printing the negative log probability with alpha 0.001\n",
    "print(f\"The log probability with alpha= 0.001 is  {negative_log_probab1}\")\n",
    "\n",
    "# printing the negative log probability with alpha 0.5\n",
    "print(f\"The log probability with alpha= 0.5 is  {negative_log_probab2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of first sentence is -69.91754047795261\n",
      "The log probability of second sentence is -113.38139766388028\n"
     ]
    }
   ],
   "source": [
    "# Function for getting the log probability of a sentence\n",
    "def sentence_log_probab(sentence, bigram_counts, alpha, vocabulary_size):\n",
    "    # Splitting the sentence into words\n",
    "    words_of_sentence = sentence.split()\n",
    "\n",
    "    #Initializing the log probability    \n",
    "    log_probability = 0.0\n",
    "\n",
    "    # Looping through the words in sentence\n",
    "    for i in range(1, len(words_of_sentence)):\n",
    "        #Calculating negative log probability\n",
    "        negative_log_probab = smoothed_biagram_probab(words_of_sentence[i], words_of_sentence[i-1], bigram_counts, alpha, vocabulary_size)\n",
    "        log_probability+= negative_log_probab\n",
    "\n",
    "    return log_probability\n",
    "\n",
    "# Initializing first sentence, getting its log probability and printing it.\n",
    "first_sentence = \"this was a really great movie but it was a little too long.\"\n",
    "log_probab1 = sentence_log_probab(first_sentence, bigram_counts, 0.1, 14813)\n",
    "print(f\"The log probability of first sentence is {log_probab1}\")\n",
    "\n",
    "# Initializing second sentence, getting its log probability and printing it.\n",
    "second_sentence = \"long too little a was it but movie great really a was this.\"\n",
    "log_probab2 = sentence_log_probab(second_sentence, bigram_counts, 0.1, 14813)\n",
    "print(f\"The log probability of second sentence is {log_probab2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood estimate for alpha as 0.001 is -3493.4581923786996\n",
      "The log likelihood estimate for alpha as 0.01 is -3924.884625422819\n",
      "The log likelihood estimate for alpha as 0.1 is -4795.08551812413\n",
      "\n",
      "The best alpha is 0.001\n"
     ]
    }
   ],
   "source": [
    "# All the alpha values\n",
    "all_alpha_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "# initializing best log chances variable\n",
    "best_log_chances = 0\n",
    "\n",
    "# setting selected alpha as 0 initially\n",
    "selected_alpha = 0\n",
    "\n",
    "# looping through all the alpha values\n",
    "for alpha in all_alpha_values:\n",
    "    approximate_log_likelihood = 0\n",
    "\n",
    "    # Applying log probability function to each sentence in the validation set\n",
    "    for i in validation_set['sentence']:\n",
    "        approximate_log_likelihood += sentence_log_probab(i, bigram_counts, alpha, vocabulary_size)\n",
    "\n",
    "    # Printing the approximate log likelihood for current alpha\n",
    "    print(f\"The log likelihood estimate for alpha as {alpha} is {approximate_log_likelihood}\")\n",
    "\n",
    "    # Check which alpha gives the highest result\n",
    "    if best_log_chances == 0 or approximate_log_likelihood > best_log_chances:\n",
    "        best_log_chances = approximate_log_likelihood\n",
    "        selected_alpha = alpha\n",
    "\n",
    "# Printing the best alpha\n",
    "print(f\"\\nThe best alpha is {selected_alpha}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
